import json
import os
import re
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage, SystemMessage
from app.core.config import settings
from app.graph2.state import AgentState

llm = ChatGroq(
    model_name="llama-3.1-8b-instant", 
    groq_api_key=settings.GROQ_API_KEY,
    temperature=0.4
)

# ============== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ==============

def extract_json(text: str):
    """ë¬¸ìì—´ì—ì„œ JSON ë¸”ë¡ì„ ì¶”ì¶œí•˜ì—¬ íŒŒì‹±í•©ë‹ˆë‹¤."""
    text = text.strip()
    if "```json" in text:
        text = text.split("```json")[1].split("```")[0].strip()
    elif "```" in text:
        text = text.split("```")[1].split("```")[0].strip()
    
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        # JSON ì „í›„ì— í…ìŠ¤íŠ¸ê°€ ì„ì—¬ ìˆì„ ê²½ìš° { } ë¥¼ ì°¾ì•„ ì‹œë„
        start = text.find('{')
        end = text.rfind('}')
        if start != -1 and end != -1:
            try:
                return json.loads(text[start:end+1])
            except:
                pass
        # ë°°ì—´ í˜•íƒœ [ ] ë„ ì‹œë„
        start = text.find('[')
        end = text.rfind(']')
        if start != -1 and end != -1:
            try:
                return json.loads(text[start:end+1])
            except:
                pass
        raise

# ============== ë…¸ë“œ í•¨ìˆ˜ë“¤ ==============

async def intent_classifier_node(state: AgentState) -> AgentState:
    """ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë¶„ë¥˜í•˜ëŠ” ë…¸ë“œ"""
    print("\n=== ì˜ë„ ë¶„ë¥˜ ì¤‘ ===")
    
    messages = state.get("messages", [])
    if not messages:
        return AgentState(intent="general_chat", current_step="intent_classifier")
    
    last_msg = messages[-1]
    if isinstance(last_msg, dict):
        user_input = last_msg.get("content", "")
    else:
        user_input = getattr(last_msg, "content", str(last_msg))
        
    system_prompt = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì…ë ¥ ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¶„ë¥˜ê¸°ì…ë‹ˆë‹¤.
    ë‹¤ìŒ ì„¸ ê°€ì§€ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:
    1. "recommend_travel": ì—¬í–‰ì§€ì™€ ê´€ë ¨ëœ ëª¨ë“  ì§ˆë¬¸, ì—¬í–‰ ì¶”ì²œ ìš”ì²­, ì—¬í–‰ ê³„íš ê´€ë ¨ ëŒ€í™” ë“±
    2. "general_chat": ì¸ì‚¬(ì•ˆë…•í•˜ì„¸ìš”, ë°˜ê°€ì›Œìš”), ì‹œìŠ¤í…œ ê´€ë ¨ ì§ˆë¬¸(ë„Œ ëˆ„êµ¬ë‹ˆ?), ì¹­ì°¬, ê°„ë‹¨í•œ ì¼ìƒ ëŒ€í™” ë“±
    3. "irrelevant_chat": ì—¬í–‰ê³¼ ì „í˜€ ìƒê´€ì—†ëŠ” ì£¼ì œ(ì •ì¹˜, ê¸°ìˆ  ì§ˆë¬¸, ìš”ë¦¬ ë ˆì‹œí”¼ ë“±)ë‚˜ ë¶€ì ì ˆí•œ ì–¸ì–´

    ì‘ë‹µì€ ì˜¤ì§ í•œ ë‹¨ì–´(recommend_travel, general_chat, irrelevant_chat)ë¡œë§Œ í•˜ì„¸ìš”."""
    
    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_input)
    ])
    
    intent = response.content.strip().lower()
    # ì—„ê²©í•œ í•„í„°ë§: ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ ëŒ€ë¹„
    if "recommend" in intent:
        intent = "recommend_travel"
    elif "general" in intent:
        intent = "general_chat"
    elif "irrelevant" in intent:
        intent = "irrelevant_chat"
    else:
        intent = "recommend_travel" # ê¸°ë³¸ê°’
        
    print(f"ë¶„ë¥˜ëœ ì˜ë„: {intent}")
    return AgentState(intent=intent, current_step="intent_classifier")


async def general_chat_node(state: AgentState) -> AgentState:
    """ê°„ë‹¨í•œ ì¸ì‚¬ë‚˜ ì¼ìƒ ëŒ€í™”ì— ì‘ë‹µí•˜ëŠ” ë…¸ë“œ"""
    print("\n=== ì¼ìƒ ëŒ€í™” ì‘ë‹µ ì¤‘ ===")
    
    messages = state.get("messages", [])
    last_msg = messages[-1]
    if isinstance(last_msg, dict):
        user_input = last_msg.get("content", "")
    else:
        user_input = getattr(last_msg, "content", str(last_msg))
        
    system_prompt = """ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì—¬í–‰ ì „ë¬¸ê°€ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
    ì‚¬ìš©ìì˜ ì¸ì‚¬ë‚˜ ê°€ë²¼ìš´ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì§§ê²Œ ëŒ€ë‹µí•˜ì„¸ìš”. 
    ê·¸ë¦¬ê³  ìì—°ìŠ¤ëŸ½ê²Œ êµ­ë‚´ ì—¬í–‰ì§€ ì¶”ì²œì´ í•„ìš”í•˜ë©´ ì–¸ì œë“  ë§ì”€í•´ë‹¬ë¼ê³  ë§ë¶™ì´ì„¸ìš”.
    ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ë”°ëœ»í•œ ë¶„ìœ„ê¸°ë¥¼ ë§Œë“œì„¸ìš”."""
    
    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_input)
    ])
    
    return AgentState(
        messages=[response.content],
        current_step="general_chat"
    )


async def irrelevant_chat_node(state: AgentState) -> AgentState:
    """ì£¼ì œì—ì„œ ë²—ì–´ë‚œ ì§ˆë¬¸ì— ëŒ€í•´ ê°€ì´ë“œë¥¼ ì£¼ëŠ” ë…¸ë“œ"""
    print("\n=== ë¶€ì í•© ëŒ€í™” ì•ˆë‚´ ì¤‘ ===")
    
    system_prompt = """ë‹¹ì‹ ì€ 'ëŒ€í•œë¯¼êµ­ êµ­ë‚´ ì—¬í–‰ ì „ë¬¸ê°€'ì…ë‹ˆë‹¤. 
    ì‚¬ìš©ìê°€ ì—¬í–‰ê³¼ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ ë¶€ì ì ˆí•œ ë§ì„ í–ˆì„ ë•Œ, 
    ì •ì¤‘í•˜ê²Œ ë‹¹ì‹ ì˜ ì—­í• ì„ ì„¤ëª…í•˜ê³  ì—¬í–‰ì— ê´€í•œ ì§ˆë¬¸ë§Œ í•´ë‹¬ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.
    ë‹¨í˜¸í•˜ì§€ë§Œ ì¹œì ˆí•œ ë§íˆ¬ë¥¼ ìœ ì§€í•˜ì„¸ìš”."""
    
    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content="ì—¬í–‰ê³¼ ìƒê´€ì—†ëŠ” ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ ë¶€ì ì ˆí•œ ëŒ€í™”ë¥¼ ì‹œë„í•¨.")
    ])
    
    return AgentState(
        messages=[response.content],
        current_step="irrelevant_chat"
    )


async def collect_preferences_node(state: AgentState) -> AgentState:
    """ì‚¬ìš©ì ì„ í˜¸ë„ ìˆ˜ì§‘ ë…¸ë“œ"""
    print("\n=== ì…ë ¥ ìˆ˜ì§‘ ì¤‘ ===")
    
    system_prompt = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ êµ­ë‚´ ì—¬í–‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì—¬í–‰ ì„ í˜¸ë„ë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”.
    ë‹¤ìŒ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤:
    - budget (ì˜ˆì‚°)
    - duration (ì—¬í–‰ ê¸°ê°„, ì¼ ë‹¨ìœ„)
    - interests (ê´€ì‹¬ì‚¬: ë¬¸í™”, ìì—°, ìŒì‹, ì•¡í‹°ë¹„í‹° ë“±)
    - travel_style (ì—¬í–‰ ìŠ¤íƒ€ì¼: íœ´ì–‘, ëª¨í—˜, ê´€ê´‘ ë“±)
    - season (ì„ í˜¸ ê³„ì ˆ ë˜ëŠ” ì—¬í–‰ ì‹œê¸°)
    - companion (ë™í–‰ì¸: í˜¼ì, ê°€ì¡±, ì—°ì¸, ì¹œêµ¬ ë“±)
    
    ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¶„ì„í•´ì„œ JSON í˜•íƒœë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
    ë¶€ì¡±í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ í•„ìš”í•œ ì§ˆë¬¸ë„ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”."""
    
    # í•„ìˆ˜ ì •ë³´ê°€ ì™„ë²½í•˜ì§€ ì•Šì€ ê²½ìš° ê³„ì†í•´ì„œ ìˆ˜ì§‘/ì—…ë°ì´íŠ¸ ì‹œë„
    required_fields = ["budget", "interests"]
    current_prefs = state.get("user_preferences", {})
    has_all = all(field in current_prefs and current_prefs[field] for field in required_fields)
    
    if not has_all:
        messages = state.get("messages", [])
        if not messages:
            return AgentState(
                user_preferences={},
                messages=["ì•ˆë…•í•˜ì„¸ìš”! ì™„ë²½í•œ ì—¬í–‰ì§€ë¥¼ ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì˜ˆì‚°, ì—¬í–‰ ê¸°ê°„, ê´€ì‹¬ì‚¬ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."],
                current_step="collect_preferences"
            )
        
        # ìœ ì €ì˜ ë§ˆì§€ë§‰ ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
        last_msg = messages[-1]
        if isinstance(last_msg, dict):
            user_input = last_msg.get("content", "")
        else:
            user_input = getattr(last_msg, "content", str(last_msg))
            
        # ê¸°ì¡´ì— ì•Œê³  ìˆëŠ” ì •ë³´ë„ í•¨ê»˜ ì „ë‹¬í•˜ì—¬ ë¬¸ë§¥ ìœ ì§€
        try:
            response = await llm.ainvoke([
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"ê¸°ì¡´ ì¶”ì¶œ ì •ë³´: {json.dumps(current_prefs, ensure_ascii=False)}\nì‚¬ìš©ì ì…ë ¥: {user_input}\n\nìƒˆë¡œìš´ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì—¬ JSONìœ¼ë¡œ ì¶œë ¥í•˜ê³ , ì—¬ì „íˆ ë¶€ì¡±í•œ ì •ë³´ëŠ” 'missing_fields'ì— ë‚˜ì—´í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ ì˜¤ì§ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.")
            ])
        except Exception as e:
            print(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return AgentState(
                messages=["ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."],
                current_step="collect_preferences"
            )

        # ì‘ë‹µ íŒŒì‹±
        try:
            result = extract_json(response.content)
            # ê¸°ì¡´ ì •ë³´ì™€ ìƒˆë¡œ ì¶”ì¶œëœ ì •ë³´ ë³‘í•©
            new_preferences = {**current_prefs}
            for k, v in result.items():
                if k != "missing_fields" and v:
                    new_preferences[k] = v
                    
            missing = result.get("missing_fields", [])
            
            # ë³‘í•©ëœ ì •ë³´ë¡œ ë‹¤ì‹œ í•œë²ˆ ëˆ„ë½ í™•ì¸
            final_missing = [f for f in required_fields if f not in new_preferences or not new_preferences[f]]
            
            if final_missing:
                return AgentState(
                    user_preferences=new_preferences,
                    messages=[f"ì¢‹ìŠµë‹ˆë‹¤! ì¶”ê°€ë¡œ ë‹¤ìŒ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤: {', '.join(final_missing)}"],
                    current_step="collect_preferences"
                )
            
            # í•„ìˆ˜ ì •ë³´ ì¶©ì¡±ë„ ê³„ì‚°
            quality = sum(1 for f in required_fields if f in new_preferences) / len(required_fields)
            
            return AgentState(
                user_preferences=new_preferences,
                info_quality_score=quality,
                messages=["ëª¨ë“  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ! ë§ì¶¤ ì—¬í–‰ì§€ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤..."],
                current_step="collect_preferences"
            )
        except Exception as e:
            print(f"íŒŒì‹± ì˜¤ë¥˜: {e}")
            return AgentState(
                messages=["ì •ë³´ë¥¼ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜ˆì‚°, ê´€ì‹¬ì‚¬(ë¬¸í™”, ìì—°, ìŒì‹ ë“±)ë¥¼ ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”."],
                current_step="collect_preferences"
            )
    
    return AgentState(current_step="collect_preferences")


async def generate_candidates_node(state: AgentState) -> AgentState:
    """ì—¬í–‰ì§€ í›„ë³´ ìƒì„± ë…¸ë“œ"""
    print("\n=== í›„ë³´ì§€ ìƒì„± ì¤‘ ===")
    
    prefs = state["user_preferences"]
    retry_count = state.get("retry_count", 0)
    
    system_prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ êµ¬ì„êµ¬ì„ì„ ê¿°ëš«ê³  ìˆëŠ” êµ­ë‚´ ì—¬í–‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    ì‚¬ìš©ìì˜ ì„ í˜¸ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ **êµ­ë‚´ ì—¬í–‰ì§€(í•œêµ­ ë‚´ ë„ì‹œ ë° ì§€ì—­)** 3-5ê°œë¥¼ ì¶”ì²œí•˜ì„¸ìš”.

    ì‚¬ìš©ì ì •ë³´:
    {json.dumps(prefs, ensure_ascii=False, indent=2)}

    {"[ì£¼ì˜] ì´ì „ ì¶”ì²œì´ ì„ í˜¸ë„ì™€ ë§ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ë²ˆì—ëŠ” ì´ì „ì— ì–¸ê¸‰í•˜ì§€ ì•Šì•˜ë˜ ìƒˆë¡œìš´ ì§€ì—­ì´ë‚˜ ë‹¤ë¥¸ í…Œë§ˆì˜ í•œêµ­ ì—¬í–‰ì§€ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”." if retry_count > 0 else ""}

    ê° ì—¬í–‰ì§€ì— ëŒ€í•´ ë‹¤ìŒì„ í¬í•¨í•˜ì„¸ìš”:
    - destination: ì—¬í–‰ì§€ ì´ë¦„ (ì˜ˆ: ì œì£¼ë„, ê²½ì£¼, ì–‘ì–‘ ë“±)
    - country: "ëŒ€í•œë¯¼êµ­"ìœ¼ë¡œ ê³ ì •
    - province: ë„ ë‹¨ìœ„ (ì˜ˆ: ê°•ì›ë„, ì „ë¼ë‚¨ë„, ì œì£¼íŠ¹ë³„ìì¹˜ë„ ë“±)
    - reason: ì¶”ì²œ ì´ìœ  (ì‚¬ìš©ìì˜ ê´€ì‹¬ì‚¬ì™€ í•´ë‹¹ ì§€ì—­ì˜ íŠ¹ì„±ì„ êµ¬ì²´ì ìœ¼ë¡œ ì—°ê²°)
    - estimated_cost: ì˜ˆìƒ ë¹„ìš© ë²”ìœ„ (ë‹¨ìœ„: ì›)
    - best_season: ìµœì  ë°©ë¬¸ ì‹œê¸°
    - highlights: ì£¼ìš” ë³¼ê±°ë¦¬/ëª…ì†Œ 3ê°€ì§€

    ì‘ë‹µì€ ë°˜ë“œì‹œ JSON ë°°ì—´ í˜•íƒœ([ ... ])ë¡œë§Œ í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""
    
    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content="ì—¬í–‰ì§€ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.")
    ])
    
    try:
        candidates = extract_json(response.content)
        print(f"ìƒì„±ëœ í›„ë³´: {len(candidates)}ê°œ")
        
        return AgentState(
            candidates=candidates,
            messages=[f"{len(candidates)}ê°œì˜ ì—¬í–‰ì§€ í›„ë³´ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤."],
            current_step="generate_candidates"
        )
    except Exception as e:
        print(f"í›„ë³´ ìƒì„± ì˜¤ë¥˜: {e}")
        return AgentState(
            candidates=[],
            validation_score=0.0,
            validation_feedback="í›„ë³´ ìƒì„± ì‹¤íŒ¨",
            messages=["í›„ë³´ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."],
            current_step="generate_candidates"
        )


async def validate_candidates_node(state: AgentState) -> AgentState:
    """í›„ë³´ì§€ í’ˆì§ˆ ê²€ì¦ ë…¸ë“œ"""
    print("\n=== í›„ë³´ ê²€ì¦ ì¤‘ ===")
    
    candidates = state["candidates"]
    prefs = state["user_preferences"]
    
    if not candidates:
        return AgentState(
            validation_score=0.0,
            validation_feedback="í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤",
            messages=["ê²€ì¦í•  í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."],
            current_step="validate_candidates"
        )
    
    system_prompt = f"""í›„ë³´ ì—¬í–‰ì§€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì„¸ìš”.
    
    ì‚¬ìš©ì ì„ í˜¸ë„:
    {json.dumps(prefs, ensure_ascii=False, indent=2)}
    
    í›„ë³´ ëª©ë¡:
    {json.dumps(candidates, ensure_ascii=False, indent=2)}
    
    ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:
    1. ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ì¼ì¹˜ë„ (ì˜ˆì‚°, ê´€ì‹¬ì‚¬, ê¸°ê°„) - 40ì 
    2. í›„ë³´ ë‹¤ì–‘ì„± (ì§€ì—­, ìŠ¤íƒ€ì¼ì˜ ë‹¤ì–‘ì„±) - 30ì 
    3. ì‹¤í˜„ ê°€ëŠ¥ì„± (ë¹„ì, ì ‘ê·¼ì„±, ì•ˆì „) - 20ì 
    4. ì •ë³´ êµ¬ì²´ì„± (ìƒì„¸í•˜ê³  ìœ ìš©í•œ ì •ë³´) - 10ì 
    
    JSONìœ¼ë¡œë§Œ ì‘ë‹µ:
    {{
        "score": 0.0-1.0 (ì†Œìˆ˜ì ),
        "feedback": "í‰ê°€ ì„¤ëª…",
        "issues": ["ë¬¸ì œì  ë¦¬ìŠ¤íŠ¸"]
    }}"""
    
    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content="í›„ë³´ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.")
    ])
    
    try:
        content = response.content.strip()
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
            
        validation = json.loads(content.strip())
        score = float(validation.get("score", 0.0))
        feedback = validation.get("feedback", "")
        
        print(f"ê²€ì¦ ì ìˆ˜: {score:.2f}")
        print(f"í”¼ë“œë°±: {feedback}")
        
        return AgentState(
            validation_score=score,
            validation_feedback=feedback,
            messages=[f"í›„ë³´ ê²€ì¦ ì™„ë£Œ (ì ìˆ˜: {score:.2f})"],
            current_step="validate_candidates"
        )
    except Exception as e:
        print(f"ê²€ì¦ ì˜¤ë¥˜: {e}")
        return AgentState(
            validation_score=0.5,
            validation_feedback="ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            messages=["ê²€ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."],
            current_step="validate_candidates"
        )

async def enrich_information_node(state: AgentState) -> AgentState:
    """ì—¬í–‰ì§€ ì •ë³´ ë³´ê°• ë…¸ë“œ"""
    print("\n=== ì •ë³´ ìˆ˜ì§‘ ì¤‘ ===")
    
    candidates = state["candidates"]
    
    # ì‹¤ì œë¡œëŠ” web_search ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    system_prompt = """ê° ì—¬í–‰ì§€ì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”:
    - weather: í˜„ì¬ ê³„ì ˆì˜ ë‚ ì”¨/ê¸°í›„ ì •ë³´
    - safety: ì•ˆì „ ì •ë³´ ë° ì£¼ì˜ì‚¬í•­
    - transport: êµí†µ ì •ë³´ (ê³µí•­, ëŒ€ì¤‘êµí†µ)
    - tips: ì—¬í–‰ íŒ (í˜„ì§€ ë¬¸í™”, ì¶”ì²œ ìŒì‹ ë“±)
    - recent_reviews: ìµœê·¼ ì—¬í–‰ì í”¼ë“œë°± ìš”ì•½
    
    ê° destinationì— ëŒ€í•´ enriched_info í•„ë“œë¥¼ ì¶”ê°€í•œ JSON ë°°ì—´ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
    ì›ë³¸ ì •ë³´ëŠ” ìœ ì§€í•˜ê³  enriched_infoë§Œ ì¶”ê°€í•˜ì„¸ìš”."""
    
    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ë‹¤ìŒ ì—¬í–‰ì§€ë“¤ì˜ ì •ë³´ë¥¼ ë³´ê°•í•˜ì„¸ìš”:\n{json.dumps(candidates, ensure_ascii=False, indent=2)}")
    ])
    
    try:
        enriched = extract_json(response.content)
        print(f"ì •ë³´ ë³´ê°• ì™„ë£Œ: {len(enriched)}ê°œ")
        
        return AgentState(
            enriched_data=enriched,
            messages=["ì—¬í–‰ì§€ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤."],
            current_step="enrich_information"
        )
    except Exception as e:
        print(f"ì •ë³´ ë³´ê°• ì˜¤ë¥˜: {e}")
        return AgentState(
            enriched_data=candidates,
            messages=["ì •ë³´ ë³´ê°•ì„ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤."],
            current_step="enrich_information"
        )


async def validate_information_node(state: AgentState) -> AgentState:
    """ìˆ˜ì§‘ëœ ì •ë³´ì˜ í’ˆì§ˆ ê²€ì¦ ë…¸ë“œ"""
    print("\n=== ì •ë³´ í’ˆì§ˆ ê²€ì¦ ì¤‘ ===")
    
    enriched = state["enriched_data"]
    
    system_prompt = """ìˆ˜ì§‘ëœ ì—¬í–‰ì§€ ì •ë³´ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì„¸ìš”.
    
    í‰ê°€ ê¸°ì¤€:
    - ì •ë³´ì˜ êµ¬ì²´ì„± (ëª¨í˜¸í•˜ì§€ ì•Šê³  êµ¬ì²´ì ì¸ê°€)
    - ìµœì‹ ì„± (ìµœê·¼ ì •ë³´ì¸ê°€)
    - ì™„ì„±ë„ (í•„ìš”í•œ ì •ë³´ê°€ ëª¨ë‘ ìˆëŠ”ê°€)
    
    JSONìœ¼ë¡œë§Œ ì‘ë‹µ:
    {
        "quality_score": 0.0-1.0,
        "assessment": "í‰ê°€ ë‚´ìš©"
    }"""
    
    # ì „ì²´ê°€ ë„ˆë¬´ í¬ë©´ ìƒ˜í”Œë§Œ ê²€ì¦
    sample = enriched[:3] if len(enriched) > 3 else enriched
    
    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ì •ë³´ë¥¼ í‰ê°€í•˜ì„¸ìš”:\n{json.dumps(sample, ensure_ascii=False, indent=2)}")
    ])
    
    try:
        content = response.content.strip()
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
            
        result = json.loads(content.strip())
        score = float(result.get("quality_score", 0.8))
        
        print(f"ì •ë³´ í’ˆì§ˆ ì ìˆ˜: {score:.2f}")
        
        return AgentState(
            info_quality_score=score,
            messages=[f"ì •ë³´ í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ (ì ìˆ˜: {score:.2f})"],
            current_step="validate_information"
        )
    except Exception as e:
        print(f"ì •ë³´ ê²€ì¦ ì˜¤ë¥˜: {e}")
        return AgentState(
            info_quality_score=0.7,
            messages=["ì •ë³´ ê²€ì¦ì„ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤."],
            current_step="validate_information"
        )

async def filter_options_node(state: AgentState) -> AgentState:
    """Hard constraintë¡œ í•„í„°ë§í•˜ëŠ” ë…¸ë“œ"""
    print("\n=== ì˜µì…˜ í•„í„°ë§ ì¤‘ ===")
    
    enriched = state["enriched_data"]
    prefs = state["user_preferences"]
    
    system_prompt = f"""ì‚¬ìš©ìì˜ í•„ìˆ˜ ì¡°ê±´ì— ë§ì§€ ì•ŠëŠ” ì—¬í–‰ì§€ë¥¼ ì œê±°í•˜ì„¸ìš”.
    
    ì‚¬ìš©ì ì¡°ê±´:
    {json.dumps(prefs, ensure_ascii=False, indent=2)}
    
    í•„í„°ë§ ê¸°ì¤€:
    - ì˜ˆì‚° ì´ˆê³¼ (ì˜ˆì‚°ì˜ 120%ë¥¼ ë„˜ìœ¼ë©´ ì œê±°)
    - ê³„ì ˆ/ì‹œê¸° ë¶€ì í•© (ì™„ì „íˆ ë§ì§€ ì•ŠëŠ” ê²½ìš°ë§Œ)
    - ì•ˆì „ ë¬¸ì œ (ì‹¬ê°í•œ ê²½ìš°ë§Œ)
    - ê¸°ê°„ ë¶€ì í•© (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´)
    
    ì í•©í•œ ì—¬í–‰ì§€ë§Œ JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ ë°°ì—´ë§Œ."""
    
    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ë‹¤ìŒ ì¤‘ ì í•©í•œ ì—¬í–‰ì§€ë§Œ ì„ íƒí•˜ì„¸ìš”:\n{json.dumps(enriched, ensure_ascii=False, indent=2)}")
    ])
    
    try:
        content = response.content.strip()
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
            
        filtered = json.loads(content.strip())
        print(f"í•„í„°ë§ ê²°ê³¼: {len(filtered)}ê°œ ë‚¨ìŒ (ì›ë³¸: {len(enriched)}ê°œ)")
        
        return AgentState(
            filtered_options=filtered,
            messages=[f"{len(filtered)}ê°œì˜ ì í•©í•œ ì—¬í–‰ì§€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."],
            current_step="filter_options"
        )
    except Exception as e:
        print(f"í•„í„°ë§ ì˜¤ë¥˜: {e}")
        return AgentState(
            filtered_options=enriched,
            messages=["í•„í„°ë§ì„ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤."],
            current_step="filter_options"
        )

async def rank_destinations_node(state: AgentState) -> AgentState:
    """ì—¬í–‰ì§€ ìˆœìœ„ ë§¤ê¸°ê¸° ë…¸ë“œ"""
    print("\n=== ì—¬í–‰ì§€ ìˆœìœ„í™” ì¤‘ ===")
    
    filtered = state["filtered_options"]
    prefs = state["user_preferences"]
    
    system_prompt = f"""ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ì í•©í•œ ì—¬í–‰ì§€ ìƒìœ„ 3ê°œë¥¼ ì„ ì •í•˜ì„¸ìš”.
    
    ì‚¬ìš©ì ì„ í˜¸ë„:
    {json.dumps(prefs, ensure_ascii=False, indent=2)}
    
    ê° ì—¬í–‰ì§€ì— ëŒ€í•´:
    - match_score (0-100): ì‚¬ìš©ì ì„ í˜¸ë„ ì¼ì¹˜ ì ìˆ˜
    - ranking_reason: ì´ ìˆœìœ„ë¥¼ ì¤€ êµ¬ì²´ì  ì´ìœ 
    
    ìƒìœ„ 3ê°œë¥¼ ranking ìˆœì„œëŒ€ë¡œ JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”."""
    
    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ë‹¤ìŒ ì—¬í–‰ì§€ë¥¼ ìˆœìœ„í™”í•˜ì„¸ìš”:\n{json.dumps(filtered, ensure_ascii=False, indent=2)}")
    ])
    
    try:
        ranked = extract_json(response.content)
        top3 = ranked[:3] if len(ranked) >= 3 else ranked
        print(f"Top {len(top3)} ì„ ì • ì™„ë£Œ")
        
        return AgentState(
            final_recommendations=top3,
            messages=["ìµœì¢… ì¶”ì²œì§€ë¥¼ ì„ ì •í–ˆìŠµë‹ˆë‹¤."],
            current_step="rank_destinations"
        )
    except Exception as e:
        print(f"ìˆœìœ„í™” ì˜¤ë¥˜: {e}")
        return AgentState(
            final_recommendations=filtered[:3],
            messages=["ìˆœìœ„í™”ë¥¼ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤."],
            current_step="rank_destinations"
        )


async def final_check_node(state: AgentState) -> AgentState:
    """ìµœì¢… ê²€ì¦ ë…¸ë“œ"""
    print("\n=== ìµœì¢… ê²€ì¦ ì¤‘ ===")
    
    recommendations = state["final_recommendations"]
    prefs = state["user_preferences"]
    
    system_prompt = f"""ìµœì¢… ì¶”ì²œì´ ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
    
    ì‚¬ìš©ì ì„ í˜¸ë„:
    {json.dumps(prefs, ensure_ascii=False, indent=2)}
    
    ì¶”ì²œ ê²°ê³¼:
    {json.dumps(recommendations, ensure_ascii=False, indent=2)}
    
    ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:
    - ì¶”ì²œ ì´ìœ ì˜ ë…¼ë¦¬ì„±
    - ì‚¬ìš©ì ë‹ˆì¦ˆ ì¶©ì¡±ë„
    - ì‹¤í˜„ ê°€ëŠ¥ì„±
    - ì •ë³´ì˜ ì™„ì„±ë„
    
    JSONìœ¼ë¡œ ì‘ë‹µ: {{"approved": true/false, "comments": "í‰ê°€"}}"""
    
    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content="ìµœì¢… ê²€ì¦í•´ì£¼ì„¸ìš”.")
    ])
    
    try:
        content = response.content.strip()
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
            
        result = json.loads(content.strip())
        approved = result.get("approved", True)
        comments = result.get("comments", "")
        
        print(f"ìµœì¢… ê²€ì¦: {'âœ“ í†µê³¼' if approved else 'âœ— ì‹¤íŒ¨'}")
        print(f"ì½”ë©˜íŠ¸: {comments}")
        
        return AgentState(
            messages=[f"ìµœì¢… ê²€ì¦: {comments}"],
            current_step="final_check"
        )
    except Exception as e:
        print(f"ìµœì¢… ê²€ì¦ ì˜¤ë¥˜: {e}")
        return AgentState(
            messages=["ìµœì¢… ê²€ì¦ì„ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤."],
            current_step="final_check"
        )

async def present_recommendations_node(state: AgentState) -> AgentState:
    """ìµœì¢… ì¶”ì²œ ì œì‹œ ë…¸ë“œ"""
    print("\n=== ì¶”ì²œ ê²°ê³¼ ì œì‹œ ===")
    
    recommendations = state["final_recommendations"]
    prefs = state["user_preferences"]
    
    system_prompt = """ì‚¬ìš©ìì—ê²Œ ë§¤ë ¥ì ìœ¼ë¡œ ì—¬í–‰ì§€ë¥¼ ì†Œê°œí•˜ì„¸ìš”.
    
    ê° ì—¬í–‰ì§€ë³„ë¡œ:
    1. ğŸŒŸ ì œëª©ê³¼ í•œ ì¤„ ìš”ì•½
    2. ğŸ’¡ ì¶”ì²œ ì´ìœ  (ì‚¬ìš©ì ì„ í˜¸ë„ì™€ ì—°ê²°)
    3. ğŸ“… ì˜ˆìƒ ì¼ì • ê°œìš”
    4. ğŸ’° ì˜ˆì‚° ê°€ì´ë“œ
    5. âœ¨ í•µì‹¬ íŒ ë° ì£¼ì˜ì‚¬í•­
    
    ì¹œê·¼í•˜ê³  ì„¤ë“ë ¥ ìˆê²Œ, ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•´ì„œ ì‘ì„±í•˜ì„¸ìš”.
    ê° ì—¬í–‰ì§€ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ê³ , ì½ê¸° ì‰½ê²Œ í¬ë§·íŒ…í•˜ì„¸ìš”."""
    
    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"""
        ì‚¬ìš©ì ì„ í˜¸ë„: {json.dumps(prefs, ensure_ascii=False)}
        
        ì¶”ì²œ ì—¬í–‰ì§€: {json.dumps(recommendations, ensure_ascii=False, indent=2)}
        
        ë§¤ë ¥ì ì¸ ì¶”ì²œì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """)
    ])
    
    final_message = f"""
{'='*60}
ğŸŒ ë‹¹ì‹ ì„ ìœ„í•œ ë§ì¶¤í˜• ì—¬í–‰ì§€ ì¶”ì²œ
{'='*60}

{response.content}

{'='*60}
ğŸ“ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì˜µì…˜ì„ ì›í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”!
{'='*60}
    """
    
    print(final_message)
    
    return AgentState(
        messages=[final_message],
        current_step="present_recommendations"
    )

def increment_retry(state: AgentState) -> AgentState:
    """ì¬ì‹œë„ ì¹´ìš´í„° ì¦ê°€"""
    new_count = state.get("retry_count", 0) + 1
    print(f"ì¬ì‹œë„ ì¹´ìš´í„° ì¦ê°€: {new_count}")
    return AgentState(
        retry_count=new_count,
        current_step="increment_retry"
    )

def increment_enrich_retry(state: AgentState) -> AgentState:
    """ì •ë³´ ë³´ê°• ì¬ì‹œë„ ì¹´ìš´í„° ì¦ê°€"""
    new_count = state.get("enrich_retry_count", 0) + 1
    print(f"ì •ë³´ ë³´ê°• ì¬ì‹œë„ ì¹´ìš´í„° ì¦ê°€: {new_count}")
    return AgentState(
        enrich_retry_count=new_count,
        current_step="increment_enrich_retry"
    )
